{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import snscrape.modules.twitter as sntwitter\n",
    "import pandas as pd\n",
    "import json\n",
    "from datetime import datetime\n",
    "import time\n",
    "import random\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import string\n",
    "import emoji\n",
    "import numpy as np\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import re\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NB train accuracy: 0.99375\n",
      "NB test accuracy: 0.8633333333333333\n",
      "NB: [1]\n",
      "NB: [2]\n",
      "NB: [0]\n"
     ]
    }
   ],
   "source": [
    "twitter_user1 = sntwitter.TwitterUserScraper(\"ILBERORTAYLIGSU\")\n",
    "twitter_user2=sntwitter.TwitterUserScraper(\"RTErdogan\")\n",
    "twitter_user3=sntwitter.TwitterUserScraper(\"ekrem_imamoglu\")\n",
    "tweets = []\n",
    "for i, tweet in enumerate(twitter_user1.get_items()):\n",
    "    if i == 1000:\n",
    "        break\n",
    "    tweets.append(tweet)  \n",
    "for i, tweet in enumerate(twitter_user2.get_items()):\n",
    "    if i == 1000:\n",
    "        break\n",
    "    tweets.append(tweet)     \n",
    "for i, tweet in enumerate(twitter_user3.get_items()):\n",
    "    if i == 1000:\n",
    "        break\n",
    "    tweets.append(tweet)       \n",
    "liste=[]\n",
    "liste1=[]\n",
    "unluler=[\"Ä°lber OrtaylÄ±\",\"Recep Tayyip ErdoÄŸan\",\"Ekrem Ä°mamoÄŸlu\"]\n",
    "\n",
    "stop_words = [\"a\", \"acaba\", \"altÄ±\", \"ama\", \"ancak\", \"artÄ±k\", \"asla\", \"aslÄ±nda\", \"az\", \"b\", \"bana\", \"bazen\", \"bazÄ±\", \"bazÄ±larÄ±\", \"bazÄ±sÄ±\", \"belki\", \"ben\", \"beni\", \"benim\", \"beÅŸ\", \"bile\", \"bir\", \"birÃ§oÄŸu\", \"birÃ§ok\", \"birÃ§oklarÄ±\", \"biri\", \"birisi\", \"birkaÃ§\", \"birkaÃ§Ä±\", \"birÅŸey\", \"birÅŸeyi\", \"biz\", \"bize\", \"bizi\", \"bizim\", \"bÃ¶yle\", \"bÃ¶ylece\", \"bu\", \"buna\", \"bunda\", \"bundan\", \"bunu\", \"bunun\", \"burada\", \"bÃ¼tÃ¼n\", \"c\", \"Ã§\", \"Ã§oÄŸu\", \"Ã§oÄŸuna\", \"Ã§oÄŸunu\", \"Ã§ok\", \"Ã§Ã¼nkÃ¼\", \"d\", \"da\", \"daha\", \"de\", \"deÄŸil\", \"demek\", \"diÄŸer\", \"diÄŸeri\", \"diÄŸerleri\", \"diye\", \"dokuz\", \"dolayÄ±\", \"dÃ¶rt\", \"e\", \"elbette\", \"en\", \"f\", \"fakat\", \"falan\", \"felan\", \"filan\", \"g\", \"gene\", \"gibi\", \"ÄŸ\", \"h\", \"hÃ¢lÃ¢\", \"hangi\", \"hangisi\", \"hani\", \"hatta\", \"hem\", \"henÃ¼z\", \"hep\", \"hepsi\", \"hepsine\", \"hepsini\", \"her\", \"her biri\", \"herkes\", \"herkese\", \"herkesi\", \"hiÃ§\", \"hiÃ§ kimse\", \"hiÃ§biri\", \"hiÃ§birine\", \"hiÃ§birini\", \"Ä±\", \"i\", \"iÃ§in\", \"iÃ§inde\", \"iki\", \"ile\", \"ise\",\n",
    "              \"iÅŸte\", \"j\", \"k\", \"kaÃ§\", \"kadar\", \"kendi\", \"kendine\", \"kendini\", \"ki\", \"kim\", \"kime\", \"kimi\", \"kimin\", \"kimisi\", \"l\", \"m\", \"madem\", \"mÄ±\", \"mÄ±\", \"mi\", \"mu\", \"mu\", \"mÃ¼\", \"mÃ¼\", \"n\", \"nasÄ±l\", \"ne\", \"ne kadar\", \"ne zaman\", \"neden\", \"nedir\", \"nerde\", \"nerede\", \"nereden\", \"nereye\", \"nesi\", \"neyse\", \"niÃ§in\", \"niye\", \"o\", \"on\", \"ona\", \"ondan\", \"onlar\", \"onlara\", \"onlardan\", \"onlarÄ±n\", \"onlarÄ±n\", \"onu\", \"onun\", \"orada\", \"oysa\", \"oysaki\", \"Ã¶\", \"Ã¶bÃ¼rÃ¼\", \"Ã¶n\", \"Ã¶nce\", \"Ã¶tÃ¼rÃ¼\", \"Ã¶yle\", \"p\", \"r\", \"raÄŸmen\", \"s\", \"sana\", \"sekiz\", \"sen\", \"senden\", \"seni\", \"senin\", \"siz\", \"sizden\", \"size\", \"sizi\", \"sizin\", \"son\", \"sonra\", \"ÅŸ\", \"ÅŸayet\", \"ÅŸey\", \"ÅŸeyden\", \"ÅŸeye\", \"ÅŸeyi\", \"ÅŸeyler\", \"ÅŸimdi\", \"ÅŸÃ¶yle\", \"ÅŸu\", \"ÅŸuna\", \"ÅŸunda\", \"ÅŸundan\", \"ÅŸunlar\", \"ÅŸunu\", \"ÅŸunun\", \"t\", \"tabi\", \"tamam\", \"tÃ¼m\", \"tÃ¼mÃ¼\", \"u\", \"Ã¼\", \"Ã¼Ã§\", \"Ã¼zere\", \"v\", \"var\", \"ve\", \"veya\", \"veyahut\", \"y\", \"ya\", \"ya da\", \"yani\", \"yedi\", \"yerine\", \"yine\", \"yoksa\", \"z\", \"zaten\",\"â¤ï¸\",\"ğŸŒ¹\",\"ğŸ‘¨ğŸ½\",  \"ğŸ¦³\",\"https\",\"co\",\"â€œ\",\"ğŸ‡¹ğŸ‡·\",\"''\",\"â€\",\"ğŸ“Œ\",\"ğŸ‡¹ğŸ‡·\",\"ğŸ‘‰\",\"ğŸ¤—\",\"ğŸ’›ğŸ’™\",\"ğŸ“\",\"ğŸ‘ğŸ‘ğŸ‘\",\"ğŸ‘‰ğŸ»\",\"ğŸ‘\",\"ğŸ˜Š\",\"ğŸ»\",\"ğŸ¶\",\"ğŸ’«\",\"ğŸ†\",\"âœ…\",\"ğŸ’™\",\"ğŸ•°\",\"ğŸŒ±\",\"ğŸ¤¼â€â™‚ï¸\",\"ğŸ…\",\"ğŸŒ¾\",\"âœ¨\",\"ğŸ¥\",\"ğŸŒ»\",\"ğŸ¬\",\"ğŸ¥\",\"ğŸ†ğŸ‡¹ğŸ‡·\",\"ğŸ“±\",\"ğŸ¥‡ğŸ‘\",\"ğŸ‘ğŸ‘\",\"ğŸ¤£\",\"â–¶ï¸\",\"ğŸ‹ğŸ‹\"]\n",
    "\n",
    "for i,k in enumerate(tweets):\n",
    "     a=tweets[i].content\n",
    "     if(i<1000):\n",
    "          liste1.append(a)  \n",
    "     if(i>=1000 and i<2000):\n",
    "        liste1.append(a)                                 \n",
    "     if(i>=2000 and i<3000):\n",
    "        liste1.append(a)     \n",
    "for i,kl in enumerate(unluler):\n",
    "     media_info = unluler[i]\n",
    "     text = media_info\n",
    "     if(media_info == \"Ä°lber OrtaylÄ±\"):\n",
    "        for k in range(1000):\n",
    "            liste.append({\n",
    "            \"unluler\": text\n",
    "        })\n",
    "     if(media_info == \"Recep Tayyip ErdoÄŸan\"):\n",
    "        for k in range(1000):\n",
    "            liste.append({\n",
    "            \"unluler\": text\n",
    "        })   \n",
    "     if(media_info == \"Ekrem Ä°mamoÄŸlu\"):\n",
    "        for k in range(1000):\n",
    "            liste.append({\n",
    "            \"unluler\": text\n",
    "        })     \n",
    "df = pd.DataFrame(liste)        \n",
    "df[\"tweetler\"]=liste1                   \n",
    "def clean(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(\"[0-9]+\", \"\", text)\n",
    "    text=text.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
    "    text = re.sub(\"â€™|â€œ|â€|â€˜|â€“|â€”\", \"\", text)\n",
    "    text=re.sub(\"ğŸ‡¹ğŸ‡·\",\"\",text)\n",
    "    text=re.sub(\"j\",\"\",text)\n",
    "    text=re.sub(\"w\",\"\",text)\n",
    "    text=re.sub(\"x\",\"\",text)\n",
    "    text=re.sub(\"q\",\"\",text)\n",
    "    text=re.sub(\"ğŸ“Œ\",\"\",text)\n",
    "    text=re.sub(\"ğŸ‘‰\",\"\",text)\n",
    "    text=re.sub(\"ğŸ¤—\",\"\",text)\n",
    "    text=re.sub(\"ğŸ’›ğŸ’™\",\"\",text)\n",
    "    text=re.sub(\"ğŸ“\",\"\",text)\n",
    "    text=re.sub(\"ğŸ‘ğŸ‘ğŸ‘\",\"\",text)\n",
    "    text=re.sub(\"ğŸ‘‰ğŸ»\",\"\",text)\n",
    "    text=re.sub(\"ğŸ‘\",\"\",text)\n",
    "    text=re.sub(\"ğŸ˜Š\",\"\",text)\n",
    "    text=re.sub(\"ğŸ»\",\"\",text)\n",
    "    text=re.sub(\"ğŸ¶\",\"\",text)\n",
    "    text=re.sub(\"ğŸ’«\",\"\",text)\n",
    "    text=re.sub(\"ğŸ†\",\"\",text)\n",
    "    text=re.sub(\"âœ…\",\"\",text)\n",
    "    text=re.sub(\"ğŸ’™\",\"\",text)\n",
    "    text=re.sub(\"ğŸ•°\",\"\",text)\n",
    "    text=re.sub(\"ğŸ“\",\"\",text)\n",
    "    text=re.sub(\"ğŸŒ±\",\"\",text)\n",
    "    text=re.sub(\"ğŸ¤¼â€â™‚ï¸\",\"\",text)\n",
    "    text=re.sub(\"ğŸ…\",\"\",text)\n",
    "    text=re.sub(\"ğŸŒ¾\",\"\",text)\n",
    "    text=re.sub(\"âœ¨\",\"\",text)\n",
    "    text=re.sub(\"ğŸ¥\",\"\",text)\n",
    "    text=re.sub(\"ğŸŒ»\",\"\",text)\n",
    "    text=re.sub(\"ğŸ¬\",\"\",text)\n",
    "    text=re.sub(\"ğŸ¥\",\"\",text)\n",
    "    text=re.sub(\"ğŸ†ğŸ‡¹ğŸ‡·\",\"\",text)\n",
    "    text=re.sub(\"ğŸ“±\",\"\",text)\n",
    "    text=re.sub(\"ğŸ¥‡\",\"\",text)\n",
    "    text=re.sub(\"ğŸ¥‡ğŸ‘\",\"\",text)\n",
    "    text=re.sub(\"ğŸ‘ğŸ‘\",\"\",text)\n",
    "    text=re.sub(\"ğŸ¤£\",\"\",text)\n",
    "    text=re.sub(\"â–¶ï¸\",\"\",text)\n",
    "    text=re.sub(\"ğŸ‹ğŸ‹\",\"\",text)\n",
    "    text=re.sub(\"httpstco\",\"\",text)\n",
    "    text=[t for t in text.split() if t not in stop_words]\n",
    "    return \" \".join(text)\n",
    "df[\"clean\"] = df.apply(lambda row: clean(row[\"tweetler\"]), axis=1)\n",
    "def etiketle(row):\n",
    "    if row[\"unluler\"] == unluler[0]:\n",
    "        return 0\n",
    "    if  row[\"unluler\"] == unluler[1]:  \n",
    "        return 1  \n",
    "    if row[\"unluler\"] == unluler[2]:\n",
    "        return 2\n",
    "df[\"labels\"] = df.apply(lambda row:  etiketle(row), axis=1)\n",
    "df.to_csv(\"sonuclar.csv\", index=False)\n",
    "X = df.clean.to_numpy()\n",
    "y = df.labels.to_numpy()\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42)\n",
    "vectorizer = TfidfVectorizer()\n",
    "X_train = vectorizer.fit_transform(X_train)\n",
    "X_test = vectorizer.transform(X_test)\n",
    "model_NB = MultinomialNB()\n",
    "model_NB.fit(X_train, y_train)\n",
    "print(\"NB train accuracy:\", model_NB.score(X_train, y_train))\n",
    "print(\"NB test accuracy:\", model_NB.score(X_test, y_test))\n",
    "lyric = \"cumhurbaÅŸkanÄ± olarak cami aÃ§Ä±lÄ±ÅŸÄ± yapÄ±yorum\" #1\n",
    "lyric1=\"istanbul bÃ¼yÃ¼kÅŸehir belediyesi olarak hizmetteyiz\" #2\n",
    "lyric2=\"bir Ã¶mÃ¼r nasÄ±l yaÅŸanÄ±r adlÄ± kitabÄ±m tÃ¼m satÄ±ÅŸ noktalarÄ±nda\" #0\n",
    "lyric_vec = vectorizer.transform([lyric])\n",
    "lyric_vec1 = vectorizer.transform([lyric1])\n",
    "lyric_vec2=vectorizer.transform([lyric2])\n",
    "print(\"NB:\", model_NB.predict(lyric_vec))\n",
    "print(\"NB:\", model_NB.predict(lyric_vec1))\n",
    "print(\"NB:\", model_NB.predict(lyric_vec2)) \n",
    "with open(\"nb.pkl\", \"wb\") as f:\n",
    "    pickle.dump(model_NB, f)\n",
    "\n",
    "# vektÃ¶rleÅŸtiriciyi diske kaydetme\n",
    "with open(\"vectorizer.pkl\", \"wb\") as f:\n",
    "    pickle.dump(vectorizer, f)\n",
    "# modeli diskten geri getirme\n",
    "mb_loaded = None\n",
    "with open(\"nb.pkl\", \"rb\") as f:\n",
    "    mb_loaded = pickle.load(f)\n",
    "\n",
    "# vektÃ¶rleÅŸtiriciyi diskten geri getirme\n",
    "vectorizer_loaded = None\n",
    "with open(\"vectorizer.pkl\", \"rb\") as f:\n",
    "    vectorizer_loaded = pickle.load(f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e3c61366b385842cabd85e253593a5aa90435b895db278116c89ba4a7d40c433"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
