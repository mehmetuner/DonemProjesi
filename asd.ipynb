{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import snscrape.modules.twitter as sntwitter\n",
    "import pandas as pd\n",
    "import json\n",
    "from datetime import datetime\n",
    "import time\n",
    "import random\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import string\n",
    "import emoji\n",
    "import numpy as np\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import re\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NB train accuracy: 0.99375\n",
      "NB test accuracy: 0.8633333333333333\n",
      "NB: [1]\n",
      "NB: [2]\n",
      "NB: [0]\n"
     ]
    }
   ],
   "source": [
    "twitter_user1 = sntwitter.TwitterUserScraper(\"ILBERORTAYLIGSU\")\n",
    "twitter_user2=sntwitter.TwitterUserScraper(\"RTErdogan\")\n",
    "twitter_user3=sntwitter.TwitterUserScraper(\"ekrem_imamoglu\")\n",
    "tweets = []\n",
    "for i, tweet in enumerate(twitter_user1.get_items()):\n",
    "    if i == 1000:\n",
    "        break\n",
    "    tweets.append(tweet)  \n",
    "for i, tweet in enumerate(twitter_user2.get_items()):\n",
    "    if i == 1000:\n",
    "        break\n",
    "    tweets.append(tweet)     \n",
    "for i, tweet in enumerate(twitter_user3.get_items()):\n",
    "    if i == 1000:\n",
    "        break\n",
    "    tweets.append(tweet)       \n",
    "liste=[]\n",
    "liste1=[]\n",
    "unluler=[\"İlber Ortaylı\",\"Recep Tayyip Erdoğan\",\"Ekrem İmamoğlu\"]\n",
    "\n",
    "stop_words = [\"a\", \"acaba\", \"altı\", \"ama\", \"ancak\", \"artık\", \"asla\", \"aslında\", \"az\", \"b\", \"bana\", \"bazen\", \"bazı\", \"bazıları\", \"bazısı\", \"belki\", \"ben\", \"beni\", \"benim\", \"beş\", \"bile\", \"bir\", \"birçoğu\", \"birçok\", \"birçokları\", \"biri\", \"birisi\", \"birkaç\", \"birkaçı\", \"birşey\", \"birşeyi\", \"biz\", \"bize\", \"bizi\", \"bizim\", \"böyle\", \"böylece\", \"bu\", \"buna\", \"bunda\", \"bundan\", \"bunu\", \"bunun\", \"burada\", \"bütün\", \"c\", \"ç\", \"çoğu\", \"çoğuna\", \"çoğunu\", \"çok\", \"çünkü\", \"d\", \"da\", \"daha\", \"de\", \"değil\", \"demek\", \"diğer\", \"diğeri\", \"diğerleri\", \"diye\", \"dokuz\", \"dolayı\", \"dört\", \"e\", \"elbette\", \"en\", \"f\", \"fakat\", \"falan\", \"felan\", \"filan\", \"g\", \"gene\", \"gibi\", \"ğ\", \"h\", \"hâlâ\", \"hangi\", \"hangisi\", \"hani\", \"hatta\", \"hem\", \"henüz\", \"hep\", \"hepsi\", \"hepsine\", \"hepsini\", \"her\", \"her biri\", \"herkes\", \"herkese\", \"herkesi\", \"hiç\", \"hiç kimse\", \"hiçbiri\", \"hiçbirine\", \"hiçbirini\", \"ı\", \"i\", \"için\", \"içinde\", \"iki\", \"ile\", \"ise\",\n",
    "              \"işte\", \"j\", \"k\", \"kaç\", \"kadar\", \"kendi\", \"kendine\", \"kendini\", \"ki\", \"kim\", \"kime\", \"kimi\", \"kimin\", \"kimisi\", \"l\", \"m\", \"madem\", \"mı\", \"mı\", \"mi\", \"mu\", \"mu\", \"mü\", \"mü\", \"n\", \"nasıl\", \"ne\", \"ne kadar\", \"ne zaman\", \"neden\", \"nedir\", \"nerde\", \"nerede\", \"nereden\", \"nereye\", \"nesi\", \"neyse\", \"niçin\", \"niye\", \"o\", \"on\", \"ona\", \"ondan\", \"onlar\", \"onlara\", \"onlardan\", \"onların\", \"onların\", \"onu\", \"onun\", \"orada\", \"oysa\", \"oysaki\", \"ö\", \"öbürü\", \"ön\", \"önce\", \"ötürü\", \"öyle\", \"p\", \"r\", \"rağmen\", \"s\", \"sana\", \"sekiz\", \"sen\", \"senden\", \"seni\", \"senin\", \"siz\", \"sizden\", \"size\", \"sizi\", \"sizin\", \"son\", \"sonra\", \"ş\", \"şayet\", \"şey\", \"şeyden\", \"şeye\", \"şeyi\", \"şeyler\", \"şimdi\", \"şöyle\", \"şu\", \"şuna\", \"şunda\", \"şundan\", \"şunlar\", \"şunu\", \"şunun\", \"t\", \"tabi\", \"tamam\", \"tüm\", \"tümü\", \"u\", \"ü\", \"üç\", \"üzere\", \"v\", \"var\", \"ve\", \"veya\", \"veyahut\", \"y\", \"ya\", \"ya da\", \"yani\", \"yedi\", \"yerine\", \"yine\", \"yoksa\", \"z\", \"zaten\",\"❤️\",\"🌹\",\"👨🏽\",  \"🦳\",\"https\",\"co\",\"“\",\"🇹🇷\",\"''\",\"”\",\"📌\",\"🇹🇷\",\"👉\",\"🤗\",\"💛💙\",\"📍\",\"👏👏👏\",\"👉🏻\",\"👏\",\"😊\",\"🎻\",\"🎶\",\"💫\",\"🏆\",\"✅\",\"💙\",\"🕰\",\"🌱\",\"🤼‍♂️\",\"🏅\",\"🌾\",\"✨\",\"🥁\",\"🌻\",\"🐬\",\"🎥\",\"🏆🇹🇷\",\"📱\",\"🥇👏\",\"👏👏\",\"🤣\",\"▶️\",\"🍋🍋\"]\n",
    "\n",
    "for i,k in enumerate(tweets):\n",
    "     a=tweets[i].content\n",
    "     if(i<1000):\n",
    "          liste1.append(a)  \n",
    "     if(i>=1000 and i<2000):\n",
    "        liste1.append(a)                                 \n",
    "     if(i>=2000 and i<3000):\n",
    "        liste1.append(a)     \n",
    "for i,kl in enumerate(unluler):\n",
    "     media_info = unluler[i]\n",
    "     text = media_info\n",
    "     if(media_info == \"İlber Ortaylı\"):\n",
    "        for k in range(1000):\n",
    "            liste.append({\n",
    "            \"unluler\": text\n",
    "        })\n",
    "     if(media_info == \"Recep Tayyip Erdoğan\"):\n",
    "        for k in range(1000):\n",
    "            liste.append({\n",
    "            \"unluler\": text\n",
    "        })   \n",
    "     if(media_info == \"Ekrem İmamoğlu\"):\n",
    "        for k in range(1000):\n",
    "            liste.append({\n",
    "            \"unluler\": text\n",
    "        })     \n",
    "df = pd.DataFrame(liste)        \n",
    "df[\"tweetler\"]=liste1                   \n",
    "def clean(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(\"[0-9]+\", \"\", text)\n",
    "    text=text.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
    "    text = re.sub(\"’|“|”|‘|–|—\", \"\", text)\n",
    "    text=re.sub(\"🇹🇷\",\"\",text)\n",
    "    text=re.sub(\"j\",\"\",text)\n",
    "    text=re.sub(\"w\",\"\",text)\n",
    "    text=re.sub(\"x\",\"\",text)\n",
    "    text=re.sub(\"q\",\"\",text)\n",
    "    text=re.sub(\"📌\",\"\",text)\n",
    "    text=re.sub(\"👉\",\"\",text)\n",
    "    text=re.sub(\"🤗\",\"\",text)\n",
    "    text=re.sub(\"💛💙\",\"\",text)\n",
    "    text=re.sub(\"📍\",\"\",text)\n",
    "    text=re.sub(\"👏👏👏\",\"\",text)\n",
    "    text=re.sub(\"👉🏻\",\"\",text)\n",
    "    text=re.sub(\"👏\",\"\",text)\n",
    "    text=re.sub(\"😊\",\"\",text)\n",
    "    text=re.sub(\"🎻\",\"\",text)\n",
    "    text=re.sub(\"🎶\",\"\",text)\n",
    "    text=re.sub(\"💫\",\"\",text)\n",
    "    text=re.sub(\"🏆\",\"\",text)\n",
    "    text=re.sub(\"✅\",\"\",text)\n",
    "    text=re.sub(\"💙\",\"\",text)\n",
    "    text=re.sub(\"🕰\",\"\",text)\n",
    "    text=re.sub(\"📍\",\"\",text)\n",
    "    text=re.sub(\"🌱\",\"\",text)\n",
    "    text=re.sub(\"🤼‍♂️\",\"\",text)\n",
    "    text=re.sub(\"🏅\",\"\",text)\n",
    "    text=re.sub(\"🌾\",\"\",text)\n",
    "    text=re.sub(\"✨\",\"\",text)\n",
    "    text=re.sub(\"🥁\",\"\",text)\n",
    "    text=re.sub(\"🌻\",\"\",text)\n",
    "    text=re.sub(\"🐬\",\"\",text)\n",
    "    text=re.sub(\"🎥\",\"\",text)\n",
    "    text=re.sub(\"🏆🇹🇷\",\"\",text)\n",
    "    text=re.sub(\"📱\",\"\",text)\n",
    "    text=re.sub(\"🥇\",\"\",text)\n",
    "    text=re.sub(\"🥇👏\",\"\",text)\n",
    "    text=re.sub(\"👏👏\",\"\",text)\n",
    "    text=re.sub(\"🤣\",\"\",text)\n",
    "    text=re.sub(\"▶️\",\"\",text)\n",
    "    text=re.sub(\"🍋🍋\",\"\",text)\n",
    "    text=re.sub(\"httpstco\",\"\",text)\n",
    "    text=[t for t in text.split() if t not in stop_words]\n",
    "    return \" \".join(text)\n",
    "df[\"clean\"] = df.apply(lambda row: clean(row[\"tweetler\"]), axis=1)\n",
    "def etiketle(row):\n",
    "    if row[\"unluler\"] == unluler[0]:\n",
    "        return 0\n",
    "    if  row[\"unluler\"] == unluler[1]:  \n",
    "        return 1  \n",
    "    if row[\"unluler\"] == unluler[2]:\n",
    "        return 2\n",
    "df[\"labels\"] = df.apply(lambda row:  etiketle(row), axis=1)\n",
    "df.to_csv(\"sonuclar.csv\", index=False)\n",
    "X = df.clean.to_numpy()\n",
    "y = df.labels.to_numpy()\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42)\n",
    "vectorizer = TfidfVectorizer()\n",
    "X_train = vectorizer.fit_transform(X_train)\n",
    "X_test = vectorizer.transform(X_test)\n",
    "model_NB = MultinomialNB()\n",
    "model_NB.fit(X_train, y_train)\n",
    "print(\"NB train accuracy:\", model_NB.score(X_train, y_train))\n",
    "print(\"NB test accuracy:\", model_NB.score(X_test, y_test))\n",
    "lyric = \"cumhurbaşkanı olarak cami açılışı yapıyorum\" #1\n",
    "lyric1=\"istanbul büyükşehir belediyesi olarak hizmetteyiz\" #2\n",
    "lyric2=\"bir ömür nasıl yaşanır adlı kitabım tüm satış noktalarında\" #0\n",
    "lyric_vec = vectorizer.transform([lyric])\n",
    "lyric_vec1 = vectorizer.transform([lyric1])\n",
    "lyric_vec2=vectorizer.transform([lyric2])\n",
    "print(\"NB:\", model_NB.predict(lyric_vec))\n",
    "print(\"NB:\", model_NB.predict(lyric_vec1))\n",
    "print(\"NB:\", model_NB.predict(lyric_vec2)) \n",
    "with open(\"nb.pkl\", \"wb\") as f:\n",
    "    pickle.dump(model_NB, f)\n",
    "\n",
    "# vektörleştiriciyi diske kaydetme\n",
    "with open(\"vectorizer.pkl\", \"wb\") as f:\n",
    "    pickle.dump(vectorizer, f)\n",
    "# modeli diskten geri getirme\n",
    "mb_loaded = None\n",
    "with open(\"nb.pkl\", \"rb\") as f:\n",
    "    mb_loaded = pickle.load(f)\n",
    "\n",
    "# vektörleştiriciyi diskten geri getirme\n",
    "vectorizer_loaded = None\n",
    "with open(\"vectorizer.pkl\", \"rb\") as f:\n",
    "    vectorizer_loaded = pickle.load(f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e3c61366b385842cabd85e253593a5aa90435b895db278116c89ba4a7d40c433"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
